{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4eb0ea0-2a7d-4b6b-946c-75bc2a83e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_setup():\n",
    "    # How to download competition data to temp folder(data) \n",
    "    # unzip it there, then symlink it like its a subdir\n",
    "    # NOTE: make sure kaggle.json is in /root/.kaggle/\n",
    "\n",
    "    #remove original symlink from this directory\n",
    "    !rm ./data\n",
    "    \n",
    "    #remove old setup files\n",
    "    !rm setup.sh\n",
    "\n",
    "    #create temp holder\n",
    "    !mkdir /root/data\n",
    "\n",
    "    #symlink it\n",
    "    !ln -s /root/data ./data\n",
    "\n",
    "    #download competition data to temp data folder\n",
    "    !cd ./data;kaggle competitions download -c paddy-disease-classification\n",
    "\n",
    "    #unzip it, -q is silent\n",
    "    !cd ./data;unzip -q paddy-disease-classification.zip\n",
    "\n",
    "    #setup dotfiles\n",
    "    !wget \"https://raw.githubusercontent.com/CNUClasses/dotfiles/master/setup.sh\";chmod 766 setup.sh;source ./setup.sh  \n",
    "    \n",
    "import os\n",
    "if(not os.path.exists('./data/train_images')):\n",
    "   !chmod 600 /root/.kaggle/kaggle.json\n",
    "   do_setup()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984bf6e1-34f9-4408-9447-220207210d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install needed libraries\n",
    "try: from path import Path\n",
    "except ModuleNotFoundError:\n",
    "    !pip install path --quiet\n",
    "    from path import Path\n",
    "try: import timm\n",
    "except ModuleNotFoundError:\n",
    "    !pip install timm --quiet\n",
    "    import timm\n",
    "try: import optuna\n",
    "except ModuleNotFoundError:\n",
    "    !pip install optuna --quiet\n",
    "    import optuna    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528a1f00-c4f2-41bf-9fc4-b48ebdcbf0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "sklearn.__version__\n",
    "import paddy_funcs_classes as pfc\n",
    "\n",
    "# autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1414b30-7a45-4364-91df-9d852abae75d",
   "metadata": {},
   "source": [
    "### CFG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fb0c93-7f68-421b-af92-c8145be53fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kaggle.com/code/hinepo/transfer-learning-with-timm-models-and-pytorch\n",
    "class CFG:\n",
    "    train_path='./data/train_images'   #get train and validation Datasets from here\n",
    "    test_path='./data/test_images'     #get test set from here\n",
    "    csv_path='./data/train.csv'\n",
    "    \n",
    "    ### split train and validation sets\n",
    "    split_fraction = 0.2\n",
    "\n",
    "    ##dataloader\n",
    "    drop_last=False\n",
    "    \n",
    "    ### model\n",
    "    model_name = 'resnet26d'#'convnext_small_in22k' # ## 'resnet50' # 'resnet34', 'resnet200d', 'efficientnet_b1_pruned', 'efficientnetv2_m', efficientnet_b7 ...  \n",
    "\n",
    "    #get a subset of data to work on(start with True until all experiments done\n",
    "    #and want to train on entire dataset\n",
    "    subsample_data=False\n",
    "    BATCH_SIZE= 8 if subsample_data else 32\n",
    "    N_EPOCHS = 4 if subsample_data else 50\n",
    "    # print_freq = 2 \n",
    "\n",
    "    momentum=0.9\n",
    "    \n",
    "    ### set only one to True\n",
    "    save_best_loss = False\n",
    "    save_best_accuracy = True\n",
    "\n",
    "    ### optimizer\n",
    "    # optimizer = 'adam'\n",
    "    # optimizer = 'adamw'\n",
    "    optimizer = 'rmsprop'  \n",
    "    # LEARNING_RATE=0.00001\n",
    "    LEARNING_RATE_MIN=0.004\n",
    "    LEARNING_RATE_MAX=0.01\n",
    "\n",
    "    random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a9e6c-82ae-4708-95ae-730eab72aa06",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835142b2-22ad-4d9f-8ea9-96c9aafd745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(CFG.csv_path)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53680f1-019d-4dca-8ac9-82078650828d",
   "metadata": {},
   "source": [
    "### EXPERIMENT ON SUBSET OF DATA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f05165b7-bb97-491a-9c44-f9799d1ab05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.subsample_data:\n",
    "    #stratify dataframe by multiple columns (see Paddy-Multi.ipynb)\n",
    "    # df, _ = train_test_split(df, test_size=0.5, random_state=0, stratify=df[['label', 'variety']])\n",
    "\n",
    "#     #get a small dataset to train on\n",
    "    df=df.iloc[:500,:]\n",
    "\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0bea5cc-f9eb-4e00-aede-5121f6412d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train=10407, length test=3469\n"
     ]
    }
   ],
   "source": [
    "# #get a list of files\n",
    "trn_val_files=pfc.get_fls(CFG.train_path)  \n",
    "tst_files=pfc.get_fls(CFG.test_path) \n",
    "print(f'Len train={len(trn_val_files)}, length test={len(tst_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8300b8-13cf-48e6-a279-1c9d9f31ab11",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ca212dc-7f0f-4359-82e4-43b573d58a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10407"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stratify dataframe by multiple columns (see Paddy-Multi.ipynb)\n",
    "train, val = train_test_split(df, test_size=CFG.split_fraction, random_state=0, stratify=df[['label', 'variety']])\n",
    "len(train) + len(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4fbcd-0c43-4393-b888-09213eb35289",
   "metadata": {},
   "source": [
    "### Get a list of transforms that the original model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed43233-cafd-43e0-9aaa-6200d5658266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': (3, 224, 224),\n",
       " 'interpolation': 'bicubic',\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'crop_pct': 0.875}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg=timm.data.resolve_data_config({}, model=CFG.model_name, verbose=True)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d771e97a-1dff-4213-8fac-449c43162910",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b710fd-c4cd-49b6-97b8-9f3c1ea4e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train images=8325, number validation=2082, and number test images=3469\n"
     ]
    }
   ],
   "source": [
    "#get transforms\n",
    "train_transforms, val_transforms=pfc.get_transforms(cfg)\n",
    "\n",
    "trn_dataset = pfc.MultiTaskDatasetTrain(CFG.train_path,df=train,transforms=train_transforms) #use train df\n",
    "val_dataset = pfc.MultiTaskDatasetTrain(CFG.train_path,df=val,transforms=val_transforms)   #use val df\n",
    "test_dataset= pfc.MultiTaskDatasetTest(CFG.test_path, transforms=val_transforms)            #test set\n",
    "\n",
    "print(f'Number train images={len(trn_dataset)}, number validation={len(val_dataset)}, and number test images={len(test_dataset)}')\n",
    "\n",
    "if(not CFG.subsample_data):\n",
    "    assert(len(trn_dataset)+len(val_dataset)==len(trn_val_files))\n",
    "    assert(len(test_dataset)==len(tst_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfea65-514e-43e5-a715-143f7c3eb15b",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "724ebefa-f7c6-4889-8b21-70962f9f57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trn_dl=DataLoader(trn_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=2,drop_last=CFG.drop_last) #drop last to avoid crash\n",
    "val_dl=DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=2,drop_last=CFG.drop_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff3ff7-5db6-431c-9ed2-c619444d5490",
   "metadata": {},
   "source": [
    "#### Sizes from DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db6200bd-72d3-4180-91f7-9ce232d5d883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 3, 224, 224])\n",
      "Labels batch shape: torch.Size([32])\n",
      "Varieties batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "imgs,lbls,varieties = next(iter(val_dl))\n",
    "print(f\"Feature batch shape: {imgs.size()}\")\n",
    "print(f\"Labels batch shape: {lbls.size()}\")\n",
    "print(f\"Varieties batch shape: {varieties.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d609aa-4c74-4310-8d4a-6becd5f994e2",
   "metadata": {},
   "source": [
    "#### Show the images <mark> is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24acc842-b5c8-4f07-9dac-51ee8cdf8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_some_images():\n",
    "    #look at original image and new one\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    img1,lbl,variety = trn_dataset[0]\n",
    "\n",
    "    # original image (PIL)\n",
    "    Image.open(trn_dataset.files[0]).show()\n",
    "\n",
    "    #see returned as a PIL image\n",
    "    import torchvision.transforms as T\n",
    "    transform = T.ToPILImage()\n",
    "    transform(img1).show()\n",
    "\n",
    "    #show as raw tensor\n",
    "    def show(img):\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "    show(img1)\n",
    "\n",
    "    mpr=mapper(df)   \n",
    "    print(f\"Label: {mpr.i_to_label[lbl]}\")\n",
    "    print(f\"Variety: {mpr.i_to_variety[variety]}\")\n",
    "    \n",
    "# show_some_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17616016-2a65-4bd5-86d9-4dceb288229f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Multi Head Model (2 output params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a22fd31-f69d-4ef1-b9dc-e616af0dc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR  \n",
    "\n",
    "class DiseaseAndTypeClassifier(nn.Module):\n",
    "    def __init__(self,tmodel):\n",
    "        '''\n",
    "        tmodel: pretrained model\n",
    "        ex:\n",
    "        model_name='resnet26d'\n",
    "        tmodel=timm.create_model(model_name, pretrained=True)\n",
    "        m1=DiseaseAndTypeClassifier(tmodel)\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.m = tmodel\n",
    "        #add an extra layer\n",
    "        # self.m.fc=nn.Linear(in_features=self.m.get_classifier().in_features,out_features=512, bias=False)\n",
    "        self.l1=nn.Linear(in_features=self.m.get_classifier().out_features, out_features=10, bias=False)  #rice type\n",
    "        self.l2=nn.Linear(in_features=self.m.get_classifier().out_features, out_features=10, bias=False)  #disease\n",
    "        \n",
    "    def forward(self,x):       \n",
    "        x=F.relu(self.m(x)) \n",
    "        label=self.l1(x)  #disease type\n",
    "        variety=self.l2(x)  #variety\n",
    "        return label,variety\n",
    "\n",
    "def get_model(model_name, min_lr,max_lr,num_epochs,trn_dl):\n",
    "    '''\n",
    "    gets a timm model and wraps in a  DiseaseAndTypeClassifier class\n",
    "    min_lr: minimum learning rate for OneCycleLR\n",
    "    max_lr: maximum learning rate for OneCycleLR\n",
    "    num_epochs: how many epochs to train for\n",
    "    trn_dl: TRAIN dataloader-used by OneCycleLR to calculate correct number of steps\n",
    "    cfg:\n",
    "    '''\n",
    "    #create the timm model\n",
    "    tmodel=timm.create_model(model_name, pretrained=True, num_classes=512,global_pool='catavgmax') \n",
    "\n",
    "    #and pass it to DiseaseAndTypeClassifier\n",
    "    m1=DiseaseAndTypeClassifier(tmodel)\n",
    "\n",
    "    #create optimizer\n",
    "    optimizer=optim.SGD(m1.parameters(),min_lr, momentum=CFG.momentum)\n",
    "\n",
    "    #create a learning rate scheduler\n",
    "    scheduler = OneCycleLR(optimizer, \n",
    "                max_lr = max_lr, # Upper learning rate boundaries in the cycle for each parameter group\n",
    "                steps_per_epoch=int(len(trn_dl)),epochs=num_epochs,\n",
    "                anneal_strategy = 'cos') # Specifies the annealing strategy\n",
    "\n",
    "    #create the learner that will train the model\n",
    "    return pfc.Learner(m1,scheduler,optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d21d1e-3444-4e00-8f5e-adc41731df4c",
   "metadata": {},
   "source": [
    "### Try and find a good learning rate, Optuna and Fastai LR finder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb4b2d-4597-496c-8e6f-c9747f006a17",
   "metadata": {},
   "source": [
    "#### Load Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1c2cc8e-2cbc-4ee0-80c8-a98b243909bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 2 µs, total: 8 µs\n",
      "Wall time: 11.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import optuna\n",
    "\n",
    "def objective(trial,trn_dl=trn_dl,val_dl=val_dl):\n",
    "    \n",
    "    #these are the parameters I want to optimize\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1)\n",
    "    \n",
    "    #get a model\n",
    "    lrn=get_model(CFG.model_name,min_lr=lr,max_lr=10*lr, num_epochs=1,trn_dl=trn_dl)\n",
    "\n",
    "    #train the model\n",
    "    lrn._trn_epoch(trn_dl)\n",
    "    \n",
    "    #see how good it is\n",
    "    acc_labl,_ = pfc.get_accuracy(lrn.m,val_dl, verbose=False)\n",
    "    \n",
    "    #get the cross validation score\n",
    "    return acc_labl\n",
    "\n",
    "#object that will optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "#start optimizing, go for 50 rounds\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('best accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf113bb-f441-431d-9fb0-6d0101472431",
   "metadata": {},
   "source": [
    "#### Fastai LR finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f769bd7-9c2e-4148-b8c0-8f1d32ccc3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore\n",
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1563a91-5888-41da-ac1c-f8bffb7dc64a",
   "metadata": {},
   "source": [
    "### Load and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65853875-f6e0-414b-965d-2e749fcc6b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 366 ms, sys: 68.1 ms, total: 434 ms\n",
      "Wall time: 327 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lrn=get_model(model_name=CFG.model_name,min_lr=CFG.LEARNING_RATE_MIN,max_lr=CFG.LEARNING_RATE_MAX, num_epochs=CFG.N_EPOCHS,trn_dl=trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9738c9-cb7a-4f6f-b60f-c245b13ed586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:err_rate_labels=0.48,   label_loss=1.41,  err_rate_varieties=0.27   varieties_loss=0.86\n",
      "valid:err_rate_labels=0.98,   label_loss=5.66,  err_rate_varieties=0.99   varieties_loss=6.98\n",
      "Epoch 0found and saving better model\n",
      "\n",
      "train:err_rate_labels=0.28,   label_loss=0.81,  err_rate_varieties=0.16   varieties_loss=0.47\n",
      "valid:err_rate_labels=0.98,   label_loss=7.22,  err_rate_varieties=0.98   varieties_loss=6.98\n",
      "train:err_rate_labels=0.23,   label_loss=0.68,  err_rate_varieties=0.12   varieties_loss=0.35\n",
      "valid:err_rate_labels=0.99,   label_loss=8.40,  err_rate_varieties=0.99   varieties_loss=8.83\n",
      "train:err_rate_labels=0.21,   label_loss=0.64,  err_rate_varieties=0.13   varieties_loss=0.37\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs=15\n",
    "\n",
    "#train the model\n",
    "lrn.learn(trn_dl,val_dl,num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d9bb3-8988-4590-95ad-1a14de0dc797",
   "metadata": {},
   "source": [
    "### Lets plot the cyclic learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792b1b3-d286-4ec5-a24e-3387eec2848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lrn.lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebae79-fc9f-4d8b-adaa-0cf6099b49d6",
   "metadata": {},
   "source": [
    "## Predict on the val set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef3cb2-b286-441c-a48a-2f246b8e88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d42993-eece-4d83-bfd1-6e13e607b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "get_accuracy(lrn.m,val_dl)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552803ec-3330-446f-8fb3-0831f8e83a68",
   "metadata": {},
   "source": [
    "## Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b192da-3e8c-42fd-86b5-6dfa8ee02d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate sorted list of files\n",
    "test_dataset=MultiTaskDatasetTest(CFG.test_path, transforms=val_transforms) \n",
    "test_dataset.files.sort()\n",
    "\n",
    "#generate a non_shuffle dataloader\n",
    "tst_dl=DataLoader(test_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=1,drop_last=False)\n",
    "\n",
    "#where is the model\n",
    "device = next(m1.parameters()).device.type\n",
    "\n",
    "#get model predictions\n",
    "labels=[]\n",
    "for imgs in tst_dl:\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    pred_lbls,_ = lrn.m(imgs)\n",
    "    \n",
    "    #get the max index\n",
    "    pred_lbls=torch.argmax(pred_lbls,dim=1).tolist()\n",
    "    labels=labels+pred_lbls   \n",
    "\n",
    "#convert indexes to labels\n",
    "new_labels=[mpr.i_to_label[i] for i in labels]\n",
    "# get a list of files\n",
    "files=[fle.split('/')[-1] for fle in test_dataset.files]   \n",
    "\n",
    "with open('./submission.csv','w') as fle:\n",
    "    fle.write('image_id,label')\n",
    "    for i in range(len(files)):\n",
    "        fle.write('\\n'+files[i]+','+new_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66059f1-ec8c-4b4c-ae34-504a3b705013",
   "metadata": {},
   "source": [
    "## Zip And Upload to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c5c7a-ae25-427e-af9c-968597e16506",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip ./submission.zip ./submission.csv\n",
    "!kaggle competitions submit -c paddy-disease-classification -f submission.zip -m \"Message\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946d6ba-9b22-480d-9c33-3ae3805140be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d854e1e-e3d4-42c8-a011-8d3c90172544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where is this model located?\n",
    "# tmodel1=timm.create_model(model_name, pretrained=True, num_classes=13,global_pool='catavgmax')  #going to replace the 13\n",
    "# next(tmodel1.parameters()).device.type\n",
    "\n",
    "#     def _one_epoch(self,dl,training):\n",
    "#         running_loss_labels = 0.0\n",
    "#         running_loss_varieties = 0.0\n",
    "#         running_err_rate_labels =0.0\n",
    "#         running_err_rate_varieties =0.0\n",
    "#         is_trn=self.m.training\n",
    "        \n",
    "#         # for i, data in (enumerate(tqdm(dl), 0)):\n",
    "#         print()\n",
    "#         for i, data in (enumerate(dl)):\n",
    "\n",
    "#             # get the inputs, labels is tuple(label, variety)\n",
    "#             imgs,lbls,varietys = data[0].to(self.device),data[1].to(self.device),data[2].to(self.device)\n",
    "           \n",
    "#             if training:\n",
    "#                 # zero the parameter gradients\n",
    "#                 self.optimizer.zero_grad()\n",
    " \n",
    "#             # forward + backward + optimize\n",
    "#             pred_lbls,pred_varieties = self.m(imgs)\n",
    "             \n",
    "#             loss_labels , loss_varieties = self.criterion(pred_lbls,pred_varieties, lbls,varietys)\n",
    "            \n",
    "#             if training:\n",
    "#                 #see https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method\n",
    "#                 loss_labels.backward(retain_graph=True)\n",
    "#                 loss_varieties.backward()\n",
    "\n",
    "#             running_loss_labels+=loss_labels.item()\n",
    "#             running_loss_varieties+=loss_varieties.item()\n",
    "#             running_err_rate_labels+=error_rate(pred_lbls,lbls)\n",
    "#             running_err_rate_varieties+=error_rate(pred_varieties,varietys)\n",
    " \n",
    "#             #adjust weights\n",
    "#             self.optimizer.step()\n",
    "\n",
    "#             if (i%CFG.print_freq==0):\n",
    "#                 state='train' if is_trn else 'valid' \n",
    "#                 print(f'{state}:err_rate_labels={(running_err_rate_labels/(i)):.2f},   label_loss={(running_loss_labels):.2f},   err_rate_varieties={(running_err_rate_varieties/(i)):.2f}   varieties_loss={(running_loss_varieties):.2f},    ', end='\\r', flush=True)\n",
    "#                 running_loss_labels = 0.0\n",
    "#                 running_loss_varieties = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338cf58-1441-466e-a44c-9e646c681446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1.train()\n",
    "# m1.eval()\n",
    "# print(f'{m1.training}, {m1.m.training}, {m1.m.fc.training}')\n",
    "\n",
    "#check shapes\n",
    "# (next(iter(trn_dl))[-2:])\n",
    "\n",
    "# m1.eval()\n",
    "# m1(trn_dataset[0][0].unsqueeze(0))\n",
    "\n",
    " #calls forward\n",
    "# m1(next(iter(trn_dl))[0]) \n",
    "# m1(val_dataset[0].unsqueeze(0))\n",
    "\n",
    "# m1(torch.randn(1,3,224,224))\n",
    "\n",
    "\n",
    "\n",
    "# criterion=DiseaseAndTypeClassifierLoss()\n",
    "# optimizer = optim.SGD(m1.parameters(), lr=0.001, momentum=0.9)\n",
    "# num_epochs=3\n",
    "# numb_batches_between_prints=1\n",
    "\n",
    "        \n",
    "        \n",
    "# for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss_labels = 0.0\n",
    "#     running_loss_varieties = 0.0\n",
    "#     num_batches=len(trn_dl)/trn_dl.batch_size\n",
    "#     m1.train()\n",
    "#     for i, data in (enumerate(tqdm(trn_dl), 0)):\n",
    "\n",
    "#         # get the inputs, labels is tuple(label, variety)\n",
    "#         imgs,lbls,varietys = data[0].to(device),data[1].to(device),data[2].to(device)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         pred_lbls,pred_varieties = m1(imgs)\n",
    "        \n",
    "#         #loss\n",
    "#         # loss = criterion(pred_lbls,pred_varieties, lbls,varietys)\n",
    "#         # loss.backward()\n",
    "#         loss_labels , loss_varieties = criterion(pred_lbls,pred_varieties, lbls,varietys)\n",
    "#         loss_labels.backward()\n",
    "#         loss_varieties.backward()\n",
    "        \n",
    "#         running_loss_labels+=loss_labels.item()\n",
    "#         running_loss_varieties+=loss_varieties.item()\n",
    "        \n",
    "#         #adjust weights\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if i%numb_batches_between_prints==0:\n",
    "#             if(\n",
    "#             print(f'label_loss={running_loss_labels/(numb_batches_between_prints*batch_size):.2f}, \\\n",
    "#                   varieties_loss={running_loss_varieties/(numb_batches_between_prints*batch_size):.2f}', end='\\r', flush=True)\n",
    "#             running_loss_labels = 0.0\n",
    "#             running_loss_varieties = 0.0\n",
    "    \n",
    "#     # m1.eval()\n",
    "#     # num_batches=len(val_dl)/batch_size\n",
    "#     # for i, data in (enumerate(tqdm(val_dl), 0)): \n",
    "\n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "# # print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c4b8e-a05e-4202-9017-d029a3674465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=np.nan\n",
    "# a\n",
    "\n",
    "# import math\n",
    "# assert (math.isnan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9419f6-8409-4012-a338-231679818127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1=timm.create_model('resnet26d', pretrained=True, num_classes=10,global_pool='catavgmax')  #going to replace the 13\n",
    "# m2=timm.create_model(CFG.model_name, pretrained=True, num_classes=10,global_pool='catavgmax')  #going to replace the 13\n",
    "\n",
    "# def get_fc(m):\n",
    "#     #when iterator is exhausted, item will hold last layer\n",
    "#     for item in m.children():\n",
    "#         pass\n",
    "#     print(type(item))\n",
    "#     if(isinstance(item,torch.nn.modules.linear.Linear)):\n",
    "#         print('Its linear')\n",
    "#     else:\n",
    "#         item= get_fc(item)   \n",
    "#     return item\n",
    "\n",
    "# fcl=get_fc(m1)\n",
    "# m2.head.fc=nn.Sequential(\n",
    "#             nn.Linear(in_features=m1.get_classifier().in_features,out_features=512, bias=False),\n",
    "#             nn.ReLU())\n",
    "\n",
    "\n",
    "\n",
    "# m11=DiseaseAndTypeClassifier(m1)\n",
    "# m22=DiseaseAndTypeClassifier(m2)\n",
    "\n",
    "# #convnext has no m1.fc, get its head this way\n",
    "# tmodel.head.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd36d6-d7fa-473a-90e9-d7ae7ee0cbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
